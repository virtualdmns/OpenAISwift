import Foundation

/// Models for the Text Completions API
@available(iOS 14.0, macOS 11.0, *)
public enum CompletionModels {
    /// Request for the text completions API
    public struct CompletionRequest: Codable {
        /// The model to use
        public let model: String
        
        /// The prompt to generate completions for
        public let prompt: String
        
        /// The maximum number of tokens to generate
        public let maxTokens: Int?
        
        /// The sampling temperature
        public let temperature: Double?
        
        /// The nucleus sampling parameter
        public let topP: Double?
        
        /// How many completions to generate
        public let n: Int?
        
        /// Whether to stream the response
        public let stream: Bool?
        
        /// The log probabilities
        public let logprobs: Int?
        
        /// Whether to include the prompt in the response
        public let echo: Bool?
        
        /// The stop sequences
        public let stop: [String]?
        
        /// The presence penalty
        public let presencePenalty: Double?
        
        /// The frequency penalty
        public let frequencyPenalty: Double?
        
        /// The best of parameter
        public let bestOf: Int?
        
        /// The logit bias
        public let logitBias: [String: Int]?
        
        /// The user identifier
        public let user: String?
        
        /// Creates a new text completion request
        /// - Parameters:
        ///   - model: The model to use (e.g., "text-davinci-003")
        ///   - prompt: The prompt to generate completions for
        ///   - maxTokens: The maximum number of tokens to generate
        ///   - temperature: The sampling temperature (0-2)
        ///   - topP: The nucleus sampling parameter (0-1)
        ///   - n: How many completions to generate
        ///   - stream: Whether to stream the response
        ///   - logprobs: The log probabilities
        ///   - echo: Whether to include the prompt in the response
        ///   - stop: The stop sequences
        ///   - presencePenalty: The presence penalty (-2 to 2)
        ///   - frequencyPenalty: The frequency penalty (-2 to 2)
        ///   - bestOf: The best of parameter
        ///   - logitBias: The logit bias
        ///   - user: The user identifier
        public init(
            model: String,
            prompt: String,
            maxTokens: Int? = nil,
            temperature: Double? = nil,
            topP: Double? = nil,
            n: Int? = nil,
            stream: Bool? = nil,
            logprobs: Int? = nil,
            echo: Bool? = nil,
            stop: [String]? = nil,
            presencePenalty: Double? = nil,
            frequencyPenalty: Double? = nil,
            bestOf: Int? = nil,
            logitBias: [String: Int]? = nil,
            user: String? = nil
        ) {
            self.model = model
            self.prompt = prompt
            self.maxTokens = maxTokens
            self.temperature = temperature
            self.topP = topP
            self.n = n
            self.stream = stream
            self.logprobs = logprobs
            self.echo = echo
            self.stop = stop
            self.presencePenalty = presencePenalty
            self.frequencyPenalty = frequencyPenalty
            self.bestOf = bestOf
            self.logitBias = logitBias
            self.user = user
        }
        
        enum CodingKeys: String, CodingKey {
            case model
            case prompt
            case maxTokens = "max_tokens"
            case temperature
            case topP = "top_p"
            case n
            case stream
            case logprobs
            case echo
            case stop
            case presencePenalty = "presence_penalty"
            case frequencyPenalty = "frequency_penalty"
            case bestOf = "best_of"
            case logitBias = "logit_bias"
            case user
        }
    }
    
    /// Response from the text completions API
    public struct CompletionResponse: Decodable {
        /// The ID of the completion
        public let id: String
        
        /// The object type
        public let object: String
        
        /// The creation timestamp
        public let created: Int
        
        /// The model used
        public let model: String
        
        /// The choices generated
        public let choices: [Choice]
        
        /// The usage statistics
        public let usage: Usage?
        
        /// A choice generated by the model
        public struct Choice: Decodable {
            /// The index of the choice
            public let index: Int
            
            /// The text generated
            public let text: String
            
            /// The log probabilities
            public let logprobs: LogProbs?
            
            /// The reason the generation stopped
            public let finishReason: String?
            
            enum CodingKeys: String, CodingKey {
                case index
                case text
                case logprobs
                case finishReason = "finish_reason"
            }
            
            /// Log probabilities for the generated text
            public struct LogProbs: Decodable {
                /// The tokens
                public let tokens: [String]?
                
                /// The token log probabilities
                public let tokenLogprobs: [Double]?
                
                /// The top log probabilities
                public let topLogprobs: [[String: Double]]?
                
                /// The text offsets
                public let textOffset: [Int]?
                
                enum CodingKeys: String, CodingKey {
                    case tokens
                    case tokenLogprobs = "token_logprobs"
                    case topLogprobs = "top_logprobs"
                    case textOffset = "text_offset"
                }
            }
        }
        
        /// Usage statistics for the request
        public struct Usage: Decodable {
            /// The number of prompt tokens used
            public let promptTokens: Int
            
            /// The number of completion tokens used
            public let completionTokens: Int
            
            /// The total number of tokens used
            public let totalTokens: Int
            
            enum CodingKeys: String, CodingKey {
                case promptTokens = "prompt_tokens"
                case completionTokens = "completion_tokens"
                case totalTokens = "total_tokens"
            }
        }
    }
}